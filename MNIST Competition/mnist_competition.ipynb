{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import model_from_json\n",
    "%matplotlib inline \n",
    "# from keras.datasets import mnist\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.set_random_seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# test data\n",
    "test_images = mnist.test.images.reshape(10000, 28, 28, 1)\n",
    "test_labels = mnist.test.labels[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Agumentation\n",
    "- image rotation\n",
    "- image width shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450000, 28, 28, 1)\n",
      "(450000, 10)\n"
     ]
    }
   ],
   "source": [
    "augmentation_size = 110000\n",
    "images = np.concatenate((mnist.train.images.reshape(55000, 28, 28, 1), mnist.validation.images.reshape(5000, 28, 28, 1)), axis=0)\n",
    "labels = np.concatenate((mnist.train.labels, mnist.validation.labels), axis=0)\n",
    "\n",
    "datagen_list = [\n",
    "                ImageDataGenerator(rotation_range=20),\n",
    "                ImageDataGenerator(rotation_range=30),\n",
    "                ImageDataGenerator(width_shift_range=0.1),\n",
    "                ImageDataGenerator(width_shift_range=0.2),\n",
    "               ]\n",
    "\n",
    "for datagen in datagen_list:\n",
    "    datagen.fit(images)\n",
    "    for image, label in datagen.flow(images, labels, batch_size=augmentation_size, shuffle=False):\n",
    "        images = np.concatenate((images, image), axis=0)\n",
    "        labels = np.concatenate((labels, label), axis=0)\n",
    "        break\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1_filter_size = 3\n",
    "model_2_filter_size = 5\n",
    "model_3_filter_size = 7\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 Architecture\n",
    "1. Convolution + Convolution + MaxPool + Dropout\n",
    "2. Convolution + Convolution + MaxPool + Dropout\n",
    "3. Convolution + MaxPool + Dropout\n",
    "4. Dense + Dropout\n",
    "5. Dense + Dropout\n",
    "6. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "450000/450000 [==============================] - 111s - loss: 0.2300 - acc: 0.9279 - val_loss: 0.0270 - val_acc: 0.9915\n",
      "Epoch 2/10\n",
      "450000/450000 [==============================] - 110s - loss: 0.1013 - acc: 0.9691 - val_loss: 0.0196 - val_acc: 0.9934\n",
      "Epoch 3/10\n",
      "450000/450000 [==============================] - 108s - loss: 0.0845 - acc: 0.9748 - val_loss: 0.0181 - val_acc: 0.9950\n",
      "Epoch 4/10\n",
      "450000/450000 [==============================] - 108s - loss: 0.0770 - acc: 0.9772 - val_loss: 0.0199 - val_acc: 0.9953\n",
      "Epoch 5/10\n",
      "450000/450000 [==============================] - 108s - loss: 0.0721 - acc: 0.9789 - val_loss: 0.0204 - val_acc: 0.9943\n",
      "Epoch 6/10\n",
      "450000/450000 [==============================] - 110s - loss: 0.0692 - acc: 0.9797 - val_loss: 0.0177 - val_acc: 0.9953\n",
      "Epoch 7/10\n",
      "450000/450000 [==============================] - 108s - loss: 0.0656 - acc: 0.9812 - val_loss: 0.0182 - val_acc: 0.9951\n",
      "Epoch 8/10\n",
      "450000/450000 [==============================] - 108s - loss: 0.0641 - acc: 0.9816 - val_loss: 0.0162 - val_acc: 0.9952\n",
      "Epoch 9/10\n",
      "450000/450000 [==============================] - 108s - loss: 0.0633 - acc: 0.9822 - val_loss: 0.0172 - val_acc: 0.9947\n",
      "Epoch 10/10\n",
      "450000/450000 [==============================] - 107s - loss: 0.0620 - acc: 0.9826 - val_loss: 0.0193 - val_acc: 0.9956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a8ad625f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential([Convolution2D(filters=64, kernel_size=(model_1_filter_size, model_1_filter_size), padding='same', activation='elu', input_shape=(28, 28, 1)), \n",
    "                     Convolution2D(filters=128, kernel_size=(model_1_filter_size, model_1_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Dropout(0.5),\n",
    "                     Convolution2D(filters=128, kernel_size=(model_1_filter_size, model_1_filter_size), padding='same', activation='elu'),\n",
    "                     Convolution2D(filters=128, kernel_size=(model_1_filter_size, model_1_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Dropout(0.5),\n",
    "                     Convolution2D(filters=128, kernel_size=(model_1_filter_size, model_1_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Dropout(0.5),\n",
    "                     Flatten(),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(0.5),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(0.5),\n",
    "                     Dense(10, activation='softmax'),\n",
    "                     ])\n",
    "model1.compile(optimizer=Adam(lr=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.fit(images, labels, batch_size=256, epochs=epochs, shuffle=True, verbose=1, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model1.to_json()\n",
    "with open(\"model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model1.save_weights(\"model1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 Architecture\n",
    "1. Convolution * 2 + MaxPool + Dropout\n",
    "2. Convolution * 2 + MaxPool + Dropout\n",
    "3. Convolution + MaxPool + Dropout\n",
    "4. Dense + Dropout\n",
    "5. Dense + Dropout\n",
    "6. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "450000/450000 [==============================] - 157s - loss: 0.2090 - acc: 0.9365 - val_loss: 0.0299 - val_acc: 0.9919\n",
      "Epoch 2/10\n",
      "450000/450000 [==============================] - 156s - loss: 0.0965 - acc: 0.9718 - val_loss: 0.0199 - val_acc: 0.9939\n",
      "Epoch 3/10\n",
      "450000/450000 [==============================] - 157s - loss: 0.0815 - acc: 0.9766 - val_loss: 0.0207 - val_acc: 0.9950\n",
      "Epoch 4/10\n",
      "450000/450000 [==============================] - 156s - loss: 0.0794 - acc: 0.9775 - val_loss: 0.0161 - val_acc: 0.9954\n",
      "Epoch 5/10\n",
      "450000/450000 [==============================] - 157s - loss: 0.0754 - acc: 0.9792 - val_loss: 0.0195 - val_acc: 0.9946\n",
      "Epoch 6/10\n",
      "450000/450000 [==============================] - 156s - loss: 0.0725 - acc: 0.9805 - val_loss: 0.0139 - val_acc: 0.9956\n",
      "Epoch 7/10\n",
      "450000/450000 [==============================] - 156s - loss: 0.0725 - acc: 0.9809 - val_loss: 0.0200 - val_acc: 0.9953\n",
      "Epoch 8/10\n",
      "450000/450000 [==============================] - 156s - loss: 0.0692 - acc: 0.9819 - val_loss: 0.0189 - val_acc: 0.9952\n",
      "Epoch 9/10\n",
      "450000/450000 [==============================] - 156s - loss: 0.0716 - acc: 0.9818 - val_loss: 0.0156 - val_acc: 0.9963\n",
      "Epoch 10/10\n",
      "450000/450000 [==============================] - 156s - loss: 0.0707 - acc: 0.9829 - val_loss: 0.0149 - val_acc: 0.9959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14ae64d9b00>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential([Convolution2D(filters=64, kernel_size=(model_2_filter_size, model_2_filter_size), padding='same', activation='elu', input_shape=(28, 28, 1)), \n",
    "                     Convolution2D(filters=128, kernel_size=(model_2_filter_size, model_2_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Dropout(0.5),\n",
    "                     Convolution2D(filters=128, kernel_size=(model_2_filter_size, model_2_filter_size), padding='same', activation='elu'),\n",
    "                     Convolution2D(filters=128, kernel_size=(model_2_filter_size, model_2_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Dropout(0.5),\n",
    "                     Convolution2D(filters=128, kernel_size=(model_2_filter_size, model_2_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Dropout(0.5),\n",
    "                     Flatten(),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(0.5),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(0.5),\n",
    "                     Dense(10, activation='softmax'),\n",
    "                     ])\n",
    "model2.compile(optimizer=Adam(lr=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(images, labels, batch_size=256, epochs=epochs, shuffle=True, verbose=1, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model2.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model2.save_weights(\"model2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 Architecture\n",
    "1. Convolution + Convolution + MaxPool + Dropout\n",
    "2. Convolution + Convolution + MaxPool + Dropout\n",
    "3. Convolution + MaxPool + Dropout\n",
    "4. Dense + Dropout\n",
    "5. Dense + Dropout\n",
    "6. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "450000/450000 [==============================] - 218s - loss: 0.2103 - acc: 0.9382 - val_loss: 0.0412 - val_acc: 0.9896\n",
      "Epoch 2/10\n",
      "450000/450000 [==============================] - 216s - loss: 0.0983 - acc: 0.9723 - val_loss: 0.0203 - val_acc: 0.9945\n",
      "Epoch 3/10\n",
      "450000/450000 [==============================] - 216s - loss: 0.0872 - acc: 0.9762 - val_loss: 0.0236 - val_acc: 0.9948\n",
      "Epoch 4/10\n",
      "450000/450000 [==============================] - 216s - loss: 0.0809 - acc: 0.9792 - val_loss: 0.0244 - val_acc: 0.9945\n",
      "Epoch 5/10\n",
      "450000/450000 [==============================] - 216s - loss: 0.0778 - acc: 0.9808 - val_loss: 0.0210 - val_acc: 0.9946\n",
      "Epoch 6/10\n",
      "450000/450000 [==============================] - 215s - loss: 0.0759 - acc: 0.9821 - val_loss: 0.0347 - val_acc: 0.9929\n",
      "Epoch 7/10\n",
      "450000/450000 [==============================] - 216s - loss: 0.0780 - acc: 0.9822 - val_loss: 0.0223 - val_acc: 0.9947\n",
      "Epoch 8/10\n",
      "450000/450000 [==============================] - 215s - loss: 0.0771 - acc: 0.9829 - val_loss: 0.0239 - val_acc: 0.9954\n",
      "Epoch 9/10\n",
      "450000/450000 [==============================] - 215s - loss: 0.0769 - acc: 0.9835 - val_loss: 0.0213 - val_acc: 0.9961\n",
      "Epoch 10/10\n",
      "450000/450000 [==============================] - 215s - loss: 0.0743 - acc: 0.9845 - val_loss: 0.0234 - val_acc: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14cdd304940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential([Convolution2D(filters=64, kernel_size=(model_3_filter_size, model_3_filter_size), padding='same', activation='elu', input_shape=(28, 28, 1)), \n",
    "                     Convolution2D(filters=128, kernel_size=(model_3_filter_size, model_3_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Dropout(0.5),\n",
    "                     Convolution2D(filters=128, kernel_size=(model_3_filter_size, model_3_filter_size), padding='same', activation='elu'),\n",
    "                     Convolution2D(filters=128, kernel_size=(model_3_filter_size, model_3_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Dropout(0.5),\n",
    "                     Convolution2D(filters=128, kernel_size=(model_3_filter_size, model_3_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=(2, 2)),\n",
    "                     Dropout(0.5),\n",
    "                     Flatten(),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(0.5),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(0.5),\n",
    "                     Dense(10, activation='softmax'),\n",
    "                     ])\n",
    "model3.compile(optimizer=Adam(lr=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.fit(images, labels, batch_size=256, epochs=epochs, shuffle=True, verbose=1, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model3.to_json()\n",
    "with open(\"model3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model3.save_weights(\"model3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "def model_open(name, test_images, test_labels):\n",
    "    json_file = open(name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(name + '.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.compile(optimizer=Adam(lr=0.0005), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    prob = loaded_model.predict_proba(test_images)\n",
    "    acc = np.mean(np.equal(np.argmax(prob, axis=1), np.argmax(test_labels, axis=1)))\n",
    "    print('\\nmodel : %s, test accuracy : %4f\\n' %(name, acc))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "10000/10000 [==============================] - 1s     \n",
      "\n",
      "model : model1, test accuracy : 0.995600\n",
      "\n",
      "Loaded model from disk\n",
      "10000/10000 [==============================] - 1s     \n",
      "\n",
      "model : model2, test accuracy : 0.995900\n",
      "\n",
      "Loaded model from disk\n",
      "10000/10000 [==============================] - 2s     \n",
      "\n",
      "model : model3, test accuracy : 0.994900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob_1 = model_open('model1', test_images, test_labels)\n",
    "prob_2 = model_open('model2', test_images, test_labels)\n",
    "prob_3 = model_open('model3', test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test accuracy :  0.9965\n"
     ]
    }
   ],
   "source": [
    "final_prob = prob_1 * 1 + prob_2 * 2  + prob_3 * 1\n",
    "final_score = np.mean(np.equal(np.argmax(final_prob, axis=1), np.argmax(test_labels, axis=1)))\n",
    "print('final test accuracy : ', final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
