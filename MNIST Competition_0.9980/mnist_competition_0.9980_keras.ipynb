{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import model_from_json\n",
    "%matplotlib inline \n",
    "# from keras.datasets import mnist\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# tf.set_random_seed(2017)\n",
    "np.random.seed(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# test data\n",
    "test_images = mnist.test.images.reshape(10000, 28, 28, 1)\n",
    "test_labels = mnist.test.labels[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Agumentation\n",
    "- image rotation\n",
    "- image width shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800000, 28, 28, 1)\n",
      "(1800000, 10)\n"
     ]
    }
   ],
   "source": [
    "augmentation_size = 440000\n",
    "images = np.concatenate((mnist.train.images.reshape(55000, 28, 28, 1), mnist.validation.images.reshape(5000, 28, 28, 1)), axis=0)\n",
    "labels = np.concatenate((mnist.train.labels, mnist.validation.labels), axis=0)\n",
    "\n",
    "datagen_list = [\n",
    "                ImageDataGenerator(rotation_range=10),\n",
    "                ImageDataGenerator(rotation_range=20),\n",
    "                ImageDataGenerator(rotation_range=30),\n",
    "                ImageDataGenerator(width_shift_range=0.1),\n",
    "                ImageDataGenerator(width_shift_range=0.2),\n",
    "                ImageDataGenerator(width_shift_range=0.3),\n",
    "               ]\n",
    "\n",
    "for datagen in datagen_list:\n",
    "    datagen.fit(images)\n",
    "    for image, label in datagen.flow(images, labels, batch_size=augmentation_size, shuffle=True, seed=2017):\n",
    "        images = np.concatenate((images, image), axis=0)\n",
    "        labels = np.concatenate((labels, label), axis=0)\n",
    "        break\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 Architecture\n",
    "1. Convolution + Convolution + MaxPool + Dropout\n",
    "2. Convolution + Convolution + MaxPool + Dropout\n",
    "3. Convolution + MaxPool + Dropout\n",
    "4. Dense + Dropout\n",
    "5. Dense + Dropout\n",
    "6. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1_filter_size = 3\n",
    "model1_conv1_filter_num_1 = 64\n",
    "model1_conv1_filter_num_2 = 128\n",
    "model1_conv1_filter_num_3 = 128\n",
    "model1_conv1_filter_num_4 = 128\n",
    "model1_conv1_filter_num_5 = 128\n",
    "model1_keep_prob = 0.5\n",
    "model1_pool_size = 2\n",
    "model1_acitivation_fn = 'elu'\n",
    "model1_output_acitivation_fn = 'softmax'\n",
    "model1_padding = 'same'\n",
    "mode1l_batch_size = 256\n",
    "model1_optimizer = Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "1800000/1800000 [==============================] - 425s - loss: 0.2764 - acc: 0.9133 - val_loss: 0.0293 - val_acc: 0.9899\n",
      "Epoch 2/20\n",
      "1800000/1800000 [==============================] - 424s - loss: 0.1080 - acc: 0.9665 - val_loss: 0.0230 - val_acc: 0.9926\n",
      "Epoch 3/20\n",
      "1800000/1800000 [==============================] - 434s - loss: 0.0770 - acc: 0.9760 - val_loss: 0.0168 - val_acc: 0.9949\n",
      "Epoch 4/20\n",
      "1800000/1800000 [==============================] - 433s - loss: 0.0622 - acc: 0.9805 - val_loss: 0.0160 - val_acc: 0.9954\n",
      "Epoch 5/20\n",
      "1800000/1800000 [==============================] - 424s - loss: 0.0524 - acc: 0.9835 - val_loss: 0.0148 - val_acc: 0.9960\n",
      "Epoch 6/20\n",
      "1800000/1800000 [==============================] - 424s - loss: 0.0463 - acc: 0.9853 - val_loss: 0.0131 - val_acc: 0.9962\n",
      "Epoch 7/20\n",
      "1800000/1800000 [==============================] - 424s - loss: 0.0416 - acc: 0.9869 - val_loss: 0.0145 - val_acc: 0.9962\n",
      "Epoch 8/20\n",
      "1800000/1800000 [==============================] - 424s - loss: 0.0382 - acc: 0.9879 - val_loss: 0.0147 - val_acc: 0.9960\n",
      "Epoch 9/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0352 - acc: 0.9887 - val_loss: 0.0130 - val_acc: 0.9960\n",
      "Epoch 10/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0332 - acc: 0.9895 - val_loss: 0.0136 - val_acc: 0.9966\n",
      "Epoch 11/20\n",
      "1800000/1800000 [==============================] - 424s - loss: 0.0314 - acc: 0.9899 - val_loss: 0.0123 - val_acc: 0.9964\n",
      "Epoch 12/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0296 - acc: 0.9906 - val_loss: 0.0123 - val_acc: 0.9967\n",
      "Epoch 13/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0283 - acc: 0.9910 - val_loss: 0.0143 - val_acc: 0.9964\n",
      "Epoch 14/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0274 - acc: 0.9913 - val_loss: 0.0133 - val_acc: 0.9966\n",
      "Epoch 15/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0264 - acc: 0.9916 - val_loss: 0.0147 - val_acc: 0.9960\n",
      "Epoch 16/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0255 - acc: 0.9918 - val_loss: 0.0142 - val_acc: 0.9970\n",
      "Epoch 17/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0248 - acc: 0.9921 - val_loss: 0.0141 - val_acc: 0.9966\n",
      "Epoch 18/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0238 - acc: 0.9925 - val_loss: 0.0145 - val_acc: 0.9967\n",
      "Epoch 19/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0236 - acc: 0.9925 - val_loss: 0.0152 - val_acc: 0.9967\n",
      "Epoch 20/20\n",
      "1800000/1800000 [==============================] - 423s - loss: 0.0231 - acc: 0.9927 - val_loss: 0.0161 - val_acc: 0.9969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e9aff05f8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential([Convolution2D(filters=model1_conv1_filter_num_1, kernel_size=(model1_filter_size, model1_filter_size), padding='same', activation='elu', input_shape=(28, 28, 1)), \n",
    "                     Convolution2D(filters=model1_conv1_filter_num_2, kernel_size=(model1_filter_size, model1_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=model1_pool_size),\n",
    "                     Dropout(model1_keep_prob),\n",
    "                     Convolution2D(filters=model1_conv1_filter_num_3, kernel_size=(model1_filter_size, model1_filter_size), padding='same', activation='elu'),\n",
    "                     Convolution2D(filters=model1_conv1_filter_num_4, kernel_size=(model1_filter_size, model1_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=model1_pool_size),\n",
    "                     Dropout(model1_keep_prob),\n",
    "                     Convolution2D(filters=model1_conv1_filter_num_5, kernel_size=(model1_filter_size, model1_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=model1_pool_size),\n",
    "                     Dropout(model1_keep_prob),\n",
    "                     Flatten(),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(model1_keep_prob),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(model1_keep_prob),\n",
    "                     Dense(10, activation=model1_output_acitivation_fn),\n",
    "                     ])\n",
    "model1.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.fit(images, labels, batch_size=256, epochs=epochs, shuffle=True, verbose=1, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model1.to_json()\n",
    "with open(\"model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model1.save_weights(\"model1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 Architecture\n",
    "1. Convolution * 2 + MaxPool + Dropout\n",
    "2. Convolution * 2 + MaxPool + Dropout\n",
    "3. Convolution + MaxPool + Dropout\n",
    "4. Dense + Dropout\n",
    "5. Dense + Dropout\n",
    "6. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2_filter_size = 5\n",
    "model2_conv1_filter_num_1 = 64\n",
    "model2_conv1_filter_num_2 = 128\n",
    "model2_conv1_filter_num_3 = 128\n",
    "model2_conv1_filter_num_4 = 128\n",
    "model2_conv1_filter_num_5 = 128\n",
    "model2_keep_prob = 0.5\n",
    "model2_pool_size = (2, 2)\n",
    "model2_acitivation_fn = 'elu'\n",
    "model2_output_acitivation_fn = 'softmax'\n",
    "model2_padding = 'same'\n",
    "mode1l_batch_size = 256\n",
    "model2_optimizer = Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "1800000/1800000 [==============================] - 607s - loss: 0.2144 - acc: 0.9333 - val_loss: 0.0209 - val_acc: 0.9936\n",
      "Epoch 2/20\n",
      "1800000/1800000 [==============================] - 606s - loss: 0.0838 - acc: 0.9742 - val_loss: 0.0170 - val_acc: 0.9947\n",
      "Epoch 3/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0570 - acc: 0.9823 - val_loss: 0.0137 - val_acc: 0.9958\n",
      "Epoch 4/20\n",
      "1800000/1800000 [==============================] - 613s - loss: 0.0442 - acc: 0.9862 - val_loss: 0.0146 - val_acc: 0.9960\n",
      "Epoch 5/20\n",
      "1800000/1800000 [==============================] - 612s - loss: 0.0360 - acc: 0.9887 - val_loss: 0.0128 - val_acc: 0.9964\n",
      "Epoch 6/20\n",
      "1800000/1800000 [==============================] - 613s - loss: 0.0313 - acc: 0.9903 - val_loss: 0.0139 - val_acc: 0.9960\n",
      "Epoch 7/20\n",
      "1800000/1800000 [==============================] - 612s - loss: 0.0276 - acc: 0.9912 - val_loss: 0.0108 - val_acc: 0.9964\n",
      "Epoch 8/20\n",
      "1800000/1800000 [==============================] - 611s - loss: 0.0251 - acc: 0.9921 - val_loss: 0.0132 - val_acc: 0.9962\n",
      "Epoch 9/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0230 - acc: 0.9927 - val_loss: 0.0120 - val_acc: 0.9974\n",
      "Epoch 10/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0214 - acc: 0.9932 - val_loss: 0.0110 - val_acc: 0.9972\n",
      "Epoch 11/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0201 - acc: 0.9937 - val_loss: 0.0125 - val_acc: 0.9966\n",
      "Epoch 12/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0188 - acc: 0.9941 - val_loss: 0.0123 - val_acc: 0.9972\n",
      "Epoch 13/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0183 - acc: 0.9943 - val_loss: 0.0100 - val_acc: 0.9972\n",
      "Epoch 14/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0174 - acc: 0.9946 - val_loss: 0.0138 - val_acc: 0.9968\n",
      "Epoch 15/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0166 - acc: 0.9948 - val_loss: 0.0136 - val_acc: 0.9968\n",
      "Epoch 16/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0160 - acc: 0.9950 - val_loss: 0.0116 - val_acc: 0.9974\n",
      "Epoch 17/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0155 - acc: 0.9952 - val_loss: 0.0152 - val_acc: 0.9959\n",
      "Epoch 18/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0151 - acc: 0.9953 - val_loss: 0.0139 - val_acc: 0.9968\n",
      "Epoch 19/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0148 - acc: 0.9955 - val_loss: 0.0140 - val_acc: 0.9971\n",
      "Epoch 20/20\n",
      "1800000/1800000 [==============================] - 610s - loss: 0.0145 - acc: 0.9956 - val_loss: 0.0128 - val_acc: 0.9972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e9abd20f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential([Convolution2D(filters=model2_conv1_filter_num_1, kernel_size=(model2_filter_size, model2_filter_size), padding='same', activation='elu', input_shape=(28, 28, 1)), \n",
    "                     Convolution2D(filters=model2_conv1_filter_num_2, kernel_size=(model2_filter_size, model2_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=model2_pool_size),\n",
    "                     Dropout(model2_keep_prob),\n",
    "                     Convolution2D(filters=model2_conv1_filter_num_3, kernel_size=(model2_filter_size, model2_filter_size), padding='same', activation='elu'),\n",
    "                     Convolution2D(filters=model2_conv1_filter_num_4, kernel_size=(model2_filter_size, model2_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=model2_pool_size),\n",
    "                     Dropout(model2_keep_prob),\n",
    "                     Convolution2D(filters=model2_conv1_filter_num_5, kernel_size=(model2_filter_size, model2_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=model2_pool_size),\n",
    "                     Dropout(model2_keep_prob),\n",
    "                     Flatten(),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(model2_keep_prob),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(model2_keep_prob),\n",
    "                     Dense(10, activation=model2_output_acitivation_fn),\n",
    "                     ])\n",
    "model2.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(images, labels, batch_size=256, epochs=epochs, shuffle=True, verbose=1, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model2.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model2.save_weights(\"model2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 Architecture\n",
    "1. Convolution + Convolution + MaxPool + Dropout\n",
    "2. Convolution + Convolution + MaxPool + Dropout\n",
    "3. Convolution + MaxPool + Dropout\n",
    "4. Dense + Dropout\n",
    "5. Dense + Dropout\n",
    "6. Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3_filter_size = 7\n",
    "model3_conv1_filter_num_1 = 64\n",
    "model3_conv1_filter_num_2 = 128\n",
    "model3_conv1_filter_num_3 = 128\n",
    "model3_conv1_filter_num_4 = 128\n",
    "model3_conv1_filter_num_5 = 128\n",
    "model3_keep_prob = 0.5\n",
    "model3_pool_size = (2, 2)\n",
    "model3_acitivation_fn = 'elu'\n",
    "model3_output_acitivation_fn = 'softmax'\n",
    "model3_padding = 'same'\n",
    "mode1l_batch_size = 256\n",
    "model3_optimizer = Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "1800000/1800000 [==============================] - 842s - loss: 0.1917 - acc: 0.9408 - val_loss: 0.0209 - val_acc: 0.9936\n",
      "Epoch 2/20\n",
      "1800000/1800000 [==============================] - 840s - loss: 0.0708 - acc: 0.9785 - val_loss: 0.0146 - val_acc: 0.9956\n",
      "Epoch 3/20\n",
      "1800000/1800000 [==============================] - 840s - loss: 0.0462 - acc: 0.9858 - val_loss: 0.0117 - val_acc: 0.9967\n",
      "Epoch 4/20\n",
      "1800000/1800000 [==============================] - 839s - loss: 0.0341 - acc: 0.9894 - val_loss: 0.0139 - val_acc: 0.9967\n",
      "Epoch 5/20\n",
      "1800000/1800000 [==============================] - 839s - loss: 0.0276 - acc: 0.9914 - val_loss: 0.0139 - val_acc: 0.9966\n",
      "Epoch 6/20\n",
      "1800000/1800000 [==============================] - 838s - loss: 0.0234 - acc: 0.9927 - val_loss: 0.0145 - val_acc: 0.9964\n",
      "Epoch 7/20\n",
      "1800000/1800000 [==============================] - 840s - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0127 - val_acc: 0.9972\n",
      "Epoch 8/20\n",
      "1800000/1800000 [==============================] - 843s - loss: 0.0186 - acc: 0.9942 - val_loss: 0.0147 - val_acc: 0.9967\n",
      "Epoch 9/20\n",
      "1800000/1800000 [==============================] - 842s - loss: 0.0171 - acc: 0.9948 - val_loss: 0.0154 - val_acc: 0.9967\n",
      "Epoch 10/20\n",
      "1800000/1800000 [==============================] - 846s - loss: 0.0156 - acc: 0.9952 - val_loss: 0.0161 - val_acc: 0.9965\n",
      "Epoch 11/20\n",
      "1800000/1800000 [==============================] - 852s - loss: 0.0148 - acc: 0.9955 - val_loss: 0.0147 - val_acc: 0.9967\n",
      "Epoch 12/20\n",
      "1800000/1800000 [==============================] - 846s - loss: 0.0140 - acc: 0.9958 - val_loss: 0.0142 - val_acc: 0.9971\n",
      "Epoch 13/20\n",
      "1800000/1800000 [==============================] - 842s - loss: 0.0132 - acc: 0.9960 - val_loss: 0.0148 - val_acc: 0.9972\n",
      "Epoch 14/20\n",
      "1800000/1800000 [==============================] - 841s - loss: 0.0127 - acc: 0.9962 - val_loss: 0.0150 - val_acc: 0.9972\n",
      "Epoch 15/20\n",
      "1800000/1800000 [==============================] - 836s - loss: 0.0124 - acc: 0.9964 - val_loss: 0.0145 - val_acc: 0.9971\n",
      "Epoch 16/20\n",
      "1800000/1800000 [==============================] - 836s - loss: 0.0120 - acc: 0.9964 - val_loss: 0.0186 - val_acc: 0.9967\n",
      "Epoch 17/20\n",
      "1800000/1800000 [==============================] - 851s - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0162 - val_acc: 0.9974\n",
      "Epoch 18/20\n",
      "1800000/1800000 [==============================] - 850s - loss: 0.0114 - acc: 0.9967 - val_loss: 0.0170 - val_acc: 0.9974\n",
      "Epoch 19/20\n",
      "1800000/1800000 [==============================] - 850s - loss: 0.0115 - acc: 0.9967 - val_loss: 0.0146 - val_acc: 0.9968\n",
      "Epoch 20/20\n",
      "1800000/1800000 [==============================] - 849s - loss: 0.0113 - acc: 0.9968 - val_loss: 0.0129 - val_acc: 0.9972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22e9b7c4898>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential([Convolution2D(filters=model3_conv1_filter_num_1, kernel_size=(model3_filter_size, model3_filter_size), padding='same', activation='elu', input_shape=(28, 28, 1)), \n",
    "                     Convolution2D(filters=model3_conv1_filter_num_2, kernel_size=(model3_filter_size, model3_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=model3_pool_size),\n",
    "                     Dropout(model3_keep_prob),\n",
    "                     Convolution2D(filters=model3_conv1_filter_num_3, kernel_size=(model3_filter_size, model3_filter_size), padding='same', activation='elu'),\n",
    "                     Convolution2D(filters=model3_conv1_filter_num_4, kernel_size=(model3_filter_size, model3_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=model3_pool_size),\n",
    "                     Dropout(model3_keep_prob),\n",
    "                     Convolution2D(filters=model3_conv1_filter_num_5, kernel_size=(model3_filter_size, model3_filter_size), padding='same', activation='elu'),\n",
    "                     MaxPooling2D(pool_size=model3_pool_size),\n",
    "                     Dropout(model3_keep_prob),\n",
    "                     Flatten(),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(model3_keep_prob),\n",
    "                     Dense(1024, activation='elu'),\n",
    "                     Dropout(model3_keep_prob),\n",
    "                     Dense(10, activation=model3_output_acitivation_fn),\n",
    "                     ])\n",
    "model3.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.fit(images, labels, batch_size=256, epochs=epochs, shuffle=True, verbose=1, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model3.to_json()\n",
    "with open(\"model3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model3.save_weights(\"model3.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "def model_open(name, test_images, test_labels):\n",
    "    json_file = open(name + '.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(name + '.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "    loaded_model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    prob = loaded_model.predict_proba(test_images)\n",
    "    acc = np.mean(np.equal(np.argmax(prob, axis=1), np.argmax(test_labels, axis=1)))\n",
    "    print('\\nmodel : %s, test accuracy : %4f\\n' %(name, acc))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "10000/10000 [==============================] - 1s     \n",
      "\n",
      "model : model1, test accuracy : 0.996900\n",
      "\n",
      "Loaded model from disk\n",
      " 9984/10000 [============================>.] - ETA: 0s\n",
      "model : model2, test accuracy : 0.997200\n",
      "\n",
      "Loaded model from disk\n",
      " 9984/10000 [============================>.] - ETA: 0s\n",
      "model : model3, test accuracy : 0.997200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob_1 = model_open('model1', test_images, test_labels)\n",
    "prob_2 = model_open('model2', test_images, test_labels)\n",
    "prob_3 = model_open('model3', test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final test accuracy : 0.998000\n"
     ]
    }
   ],
   "source": [
    "final_prob = (prob_1 * 1 + prob_2 * 2  + prob_3 * 3) / 6\n",
    "final_score = np.mean(np.equal(np.argmax(final_prob, axis=1), np.argmax(test_labels, axis=1)))\n",
    "print('final test accuracy : %4f' % final_score)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
